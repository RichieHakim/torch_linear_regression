{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_linear_regression as tlr\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data for regression\n",
    "X, Y = sklearn.datasets.make_regression(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    n_informative=10,\n",
    "    bias=2,\n",
    "    noise=50,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7819072511229945\n"
     ]
    }
   ],
   "source": [
    "## Create model\n",
    "model_ols = tlr.OLS()\n",
    "## Fit model\n",
    "model_ols.fit(X=X, y=Y)\n",
    "## Predict\n",
    "Y_pred = model_ols.predict(X)\n",
    "## Score model\n",
    "score = model_ols.score(X=X, y=Y)\n",
    "print(f\"R^2: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, prefit model coefficients are the same as the model coefficients!\n"
     ]
    }
   ],
   "source": [
    "## Create model\n",
    "model_ols_pf = tlr.OLS(prefit_X=X)\n",
    "## Fit model\n",
    "model_ols_pf.fit(X=X, y=Y)  ## You should still pass X again even though it is prefit\n",
    "## Predict\n",
    "Y_pred = model_ols_pf.predict(X)\n",
    "## Check that coefficients are the same\n",
    "if np.allclose(model_ols.coef_, model_ols_pf.coef_):\n",
    "    print(\"All good, prefit model coefficients are the same as the model coefficients!\")\n",
    "else:\n",
    "    print(\"Uh oh, prefit model coefficients are different from the model coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefitting is useful when you want to fit some `X` data to many different `y` data. By passing the `X` matrix into the `prefit` argument in the initialization you can avoid recomputing the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken without prefit: 3.5273141860961914\n",
      "Time taken with prefit: 0.10075020790100098\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "## Run a speed test\n",
    "\n",
    "## Generate data for regression\n",
    "X, Y = sklearn.datasets.make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=200,\n",
    "    n_informative=100,\n",
    "    n_targets=100,\n",
    "    bias=2,\n",
    "    noise=50,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "## Fit without prefit\n",
    "tic_multi = time.time()\n",
    "model_ols_multi = tlr.OLS()\n",
    "for y_i in Y.T:\n",
    "    model_ols_multi.fit(X=X, y=y_i)\n",
    "toc_multi = time.time()\n",
    "\n",
    "## Fit with prefit\n",
    "tic_multi_pf = time.time()\n",
    "model_ols_multi_pf = tlr.OLS(prefit_X=X)\n",
    "for y_i in Y.T:\n",
    "    model_ols_multi_pf.fit(X=X, y=y_i)\n",
    "toc_multi_pf = time.time()\n",
    "\n",
    "## Check times\n",
    "print(f\"Time taken without prefit: {toc_multi - tic_multi}\")\n",
    "print(f\"Time taken with prefit: {toc_multi_pf - tic_multi_pf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use numpy or torch arrays as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, numpy model coefficients are the same as the torch model coefficients!\n"
     ]
    }
   ],
   "source": [
    "## Make numpy and torch models\n",
    "model_ols_np = tlr.OLS()\n",
    "model_ols_np.fit(X=np.array(X), y=np.array(Y))\n",
    "\n",
    "model_ols_torch = tlr.OLS()\n",
    "model_ols_torch.fit(X=torch.as_tensor(X), y=torch.as_tensor(Y))\n",
    "\n",
    "## Check that coefficients are the same\n",
    "if np.allclose(model_ols_np.coef_, model_ols_torch.coef_.numpy()):\n",
    "    print(\"All good, numpy model coefficients are the same as the torch model coefficients!\")\n",
    "else:\n",
    "    print(\"Uh oh, numpy model coefficients are different from the torch model coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model coefficients device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "## Create model\n",
    "model_ols_gpu = tlr.OLS()\n",
    "model_ols_gpu.fit(X=torch.as_tensor(X, device=device), y=torch.as_tensor(Y, device=device))\n",
    "\n",
    "## Check that coefficients are on the same device\n",
    "print(f\"Model coefficients device: {model_ols_gpu.coef_.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn and PyTorch compatibility\n",
    "The models are subclassed from `sklearn.base.BaseEstimator` and `torch.nn.Module` so they can be used in both frameworks.\n",
    "\n",
    "##### From sklearn's `BaseEstimator`:\n",
    "\n",
    "Methods:\n",
    "- `fit`\n",
    "- `predict`\n",
    "- `fit_predict`\n",
    "- `score`\n",
    "  \n",
    "Attributes:\n",
    "- `coef_`\n",
    "- `intercept_`\n",
    "\n",
    "##### From PyTorch's `torch.nn.Module`:\n",
    "- `forward`\n",
    "\n",
    "Since the model is just matrix multiplication, addition, and possibly an SVD, you can treat the models' `fit` and `predict` methods as layers in a network and use them in a PyTorch pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a new model\n",
    "model_ols_2 = tlr.OLS()\n",
    "model_ols_2.fit(X=X, y=Y)\n",
    "\n",
    "## Predict using the `forward` method\n",
    "## All three methods should give the same exact result\n",
    "Y_pred_predict = model_ols_2.predict(X)\n",
    "Y_pred_call = model_ols_2(X)\n",
    "Y_pred_forward = model_ols_2.forward(X)\n",
    "\n",
    "assert np.allclose(Y_pred_predict, Y_pred_call)\n",
    "assert np.allclose(Y_pred_predict, Y_pred_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final intercept parameter: 501.08172607421875\n",
      "True intercept parameter: 500\n"
     ]
    }
   ],
   "source": [
    "## Let's make an OLS model without an intercept and use backpropagation to fit an intercept parameter\n",
    "## Generate data for regression\n",
    "X, Y = sklearn.datasets.make_regression(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    noise=50,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "## Add a known bias to the data\n",
    "bias = 500  ## Set the bias to a known value. The param_intercept should converge to this value\n",
    "X = torch.as_tensor(X)\n",
    "Y = torch.as_tensor(Y) + bias\n",
    "\n",
    "## Create model with fit_intercept=False\n",
    "model_ols_noInt = tlr.OLS(prefit_X=X, fit_intercept=False)\n",
    "## Create a parameter for the intercept that we will optimize\n",
    "param_intercept = torch.nn.Parameter(torch.ones(1) * 1, requires_grad=True)\n",
    "\n",
    "## Fit the model\n",
    "fn_loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([param_intercept], lr=1e-1)\n",
    "losses = []\n",
    "vals = []\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    Y_pred = model_ols_noInt.fit_predict(X=X, y=Y - param_intercept)\n",
    "    # Y_pred = X @ (torch.linalg.inv(X.T @ X) @ X.T @ (Y - param_intercept))  ## This is equivalent to the line above\n",
    "    loss = fn_loss(Y_pred, Y - param_intercept)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    vals.append(param_intercept.item())\n",
    "\n",
    "## Print the final value of the intercept parameter\n",
    "print(f\"Final intercept parameter: {vals[-1]}\")\n",
    "print(f\"True intercept parameter: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlr.OLS(\n",
    "    fit_intercept=True,\n",
    "    prefit_X=None,\n",
    ")\n",
    "tlr.Ridge(\n",
    "    alpha=1e4,\n",
    "    fit_intercept=True,\n",
    "    prefit_X=None,\n",
    ")\n",
    "tlr.ReducedRankRidgeRegression(\n",
    "    rank=5,\n",
    "    alpha=1e4,\n",
    "    fit_intercept=True,\n",
    "    prefit_X=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enjoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
